{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronpedia_followup\n",
    "\n",
    "Here are two possibly-weird things I noticed when playing with the Bloom SAEs on Neuronpedia:\n",
    "\n",
    "1. There are 'starts with letter k' and 'starts with letter z' features that propagate down through the layers, starting with layer 1. (0-indexed). Possibly this has an obvious root cause, but I don't currently understand why this would be. Can we automatically identify these features and group them? Possibly this makes the space of features a human must crawl smaller.\n",
    "2. Lots of the layer-0 features have weird top postive logits: GoldMagikarp (a known tokenizer thing), but also Lumpur, CLASSIFIED, NetMessage. What's going on? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import plotly.express as px\n",
    "\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from transformer_lens import utils\n",
    "from typing import Dict\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from sae_lens.training.session_loader import LMSparseAutoencoderSessionloader\n",
    "# from sae_lens.analysis.visualizer.data_fns import get_feature_data, FeatureData\n",
    "from sae_vis.data_fetching_fns import get_feature_data, FeatureData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempting to group and track common features across layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at why these strange tokens appear so frequently\n",
    "\n",
    "Most notably, CLASSIFIED, NetMessage, largeDownload, GoldMagikarp, Lumpur.\n",
    "\n",
    "Googling reveals known weirdness with some of these: https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation. Notably with longer versions of these words.\n",
    "\n",
    "Update - lots of these are indeed 'weird tokens' - natureconservancy, NetMessage, Streamer, GoldMagikarp, largeDownload, NetMessage. But regardless, learning why it's always them coming up will be useful, and they should probably be excluded, for no other reason than because they destroy the autointerps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repro of the SAELens notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lifting from https://github.com/jbloomAus/SAELens/blob/main/tutorials/evaluating_your_sae.ipynb\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the SAE for layer 0 (the one with the Magikarps)\n",
    "\n",
    "REPO_ID = \"jbloom/GPT2-Small-SAEs\"\n",
    "layer = 0  # any layer from 0 - 11 works here\n",
    "FILENAME = f\"final_sparse_autoencoder_gpt2-small_blocks.{layer}.hook_resid_pre_24576.pt\"\n",
    "\n",
    "path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then load the SAE, dataset and model using the session loader\n",
    "model, sparse_autoencoders, activation_store = (\n",
    "    LMSparseAutoencoderSessionloader.load_session_from_pretrained(path=path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sae in enumerate(sparse_autoencoders):\n",
    "    hyp = sae.cfg\n",
    "    print(\n",
    "        f\"{i}: Layer {hyp.hook_point_layer}, p_norm {hyp.lp_norm}, alpha {hyp.l1_coefficient}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick which sae you wnat to evaluate. Default is 0\n",
    "sparse_autoencoder = list(sparse_autoencoders)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the Autoencoder\n",
    "\n",
    "sparse_autoencoder.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "with torch.no_grad():\n",
    "    batch_tokens = activation_store.get_batch_tokens()\n",
    "    _, cache = model.run_with_cache(batch_tokens, prepend_bos=True)\n",
    "    sae_out, feature_acts, loss, mse_loss, l1_loss, _ = sparse_autoencoder(\n",
    "        cache[sparse_autoencoder.cfg.hook_point]\n",
    "    )\n",
    "    del cache\n",
    "\n",
    "    # ignore the bos token, get the number of features that activated in each token, averaged accross batch and position\n",
    "    l0 = (feature_acts[:, 1:] > 0).float().sum(-1).detach()\n",
    "    print(\"average l0\", l0.mean().item())\n",
    "    px.histogram(l0.flatten().cpu().numpy()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we want to do a reconstruction test.\n",
    "def reconstr_hook(activation, hook, sae_out):\n",
    "    return sae_out\n",
    "\n",
    "\n",
    "def zero_abl_hook(activation, hook):\n",
    "    return torch.zeros_like(activation)\n",
    "\n",
    "\n",
    "print(\"Orig\", model(batch_tokens, return_type=\"loss\").item())\n",
    "print(\n",
    "    \"reconstr\",\n",
    "    model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        fwd_hooks=[\n",
    "            (\n",
    "                utils.get_act_name(\"resid_pre\", 10),\n",
    "                partial(reconstr_hook, sae_out=sae_out),\n",
    "            )\n",
    "        ],\n",
    "        return_type=\"loss\",\n",
    "    ).item(),\n",
    ")\n",
    "print(\n",
    "    \"Zero\",\n",
    "    model.run_with_hooks(\n",
    "        batch_tokens,\n",
    "        return_type=\"loss\",\n",
    "        fwd_hooks=[(utils.get_act_name(\"resid_pre\", 10), zero_abl_hook)],\n",
    "    ).item(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific Capability Test\n",
    "example_prompt = \"When John and Mary went to the shops, John gave the bag to\"\n",
    "example_answer = \" Mary\"\n",
    "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)\n",
    "\n",
    "logits, cache = model.run_with_cache(example_prompt, prepend_bos=True)\n",
    "tokens = model.to_tokens(example_prompt)\n",
    "sae_out, feature_acts, loss, mse_loss, l1_loss, _ = sparse_autoencoder(\n",
    "    cache[sparse_autoencoder.cfg.hook_point]\n",
    ")\n",
    "\n",
    "\n",
    "def reconstr_hook(activations, hook, sae_out):\n",
    "    return sae_out\n",
    "\n",
    "\n",
    "def zero_abl_hook(mlp_out, hook):\n",
    "    return torch.zeros_like(mlp_out)\n",
    "\n",
    "\n",
    "hook_point = sparse_autoencoder.cfg.hook_point\n",
    "\n",
    "print(\"Orig\", model(tokens, return_type=\"loss\").item())\n",
    "print(\n",
    "    \"reconstr\",\n",
    "    model.run_with_hooks(\n",
    "        tokens,\n",
    "        fwd_hooks=[\n",
    "            (\n",
    "                hook_point,\n",
    "                partial(reconstr_hook, sae_out=sae_out),\n",
    "            )\n",
    "        ],\n",
    "        return_type=\"loss\",\n",
    "    ).item(),\n",
    ")\n",
    "print(\n",
    "    \"Zero\",\n",
    "    model.run_with_hooks(\n",
    "        tokens,\n",
    "        return_type=\"loss\",\n",
    "        fwd_hooks=[(hook_point, zero_abl_hook)],\n",
    "    ).item(),\n",
    ")\n",
    "\n",
    "\n",
    "with model.hooks(\n",
    "    fwd_hooks=[\n",
    "        (\n",
    "            hook_point,\n",
    "            partial(reconstr_hook, sae_out=sae_out),\n",
    "        )\n",
    "    ]\n",
    "):\n",
    "    utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating feature interfaces\n",
    "vals, inds = torch.topk(feature_acts[0, -1].detach().cpu(), 10)\n",
    "px.bar(x=[str(i) for i in inds], y=vals).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sae_vis.data_config_classes import (\n",
    "    # ActsHistogramConfig,\n",
    "    # Column,\n",
    "    # FeatureTablesConfig,\n",
    "    SaeVisConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DASHBOARD_FOLDER = 'dashboards'\n",
    "\n",
    "\n",
    "vocab_dict = model.tokenizer.vocab\n",
    "vocab_dict = {\n",
    "    v: k.replace(\"Ä \", \" \").replace(\"\\n\", \"\\\\n\") for k, v in vocab_dict.items()\n",
    "}\n",
    "\n",
    "vocab_dict_filepath = Path(os.getcwd()) / \"vocab_dict.json\"\n",
    "if not vocab_dict_filepath.exists():\n",
    "    with open(vocab_dict_filepath, \"w\") as f:\n",
    "        json.dump(vocab_dict, f)\n",
    "\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "data = load_dataset(\n",
    "    \"NeelNanda/c4-code-20k\", split=\"train\"\n",
    ")  # currently use this dataset to avoid deal with tokenization while streaming\n",
    "tokenized_data = utils.tokenize_and_concatenate(data, model.tokenizer, max_length=128)\n",
    "tokenized_data = tokenized_data.shuffle(42)\n",
    "all_tokens = tokenized_data[\"tokens\"]\n",
    "\n",
    "\n",
    "# Currently, don't think much more time can be squeezed out of it. Maybe the best saving would be to\n",
    "# make the entire sequence indexing parallelized, but that's possibly not worth it right now.\n",
    "\n",
    "max_batch_size = 512\n",
    "total_batch_size = 4096 * 5\n",
    "feature_idx = list(inds.flatten().cpu().numpy())\n",
    "# max_batch_size = 512\n",
    "# total_batch_size = 16384\n",
    "# feature_idx = list(range(1000))\n",
    "\n",
    "tokens = all_tokens[:total_batch_size]\n",
    "\n",
    "\n",
    "feature_vis_params = SaeVisConfig(\n",
    "    hook_point=sparse_autoencoder.cfg.hook_point,\n",
    "    minibatch_size_features=256,\n",
    "    minibatch_size_tokens=64,\n",
    "    features=feature_idx,\n",
    "    verbose=True,\n",
    "    # feature_centric_layout=layout,\n",
    ")\n",
    "\n",
    "# feature_idx=feature_idx,\n",
    "#     max_batch_size=max_batch_size,\n",
    "#     left_hand_k=3,\n",
    "#     buffer=(5, 5),\n",
    "#     n_groups=10,\n",
    "#     first_group_size=20,\n",
    "#     other_groups_size=5,\n",
    "\n",
    "\n",
    "\n",
    "feature_data: Dict[int, FeatureData] = get_feature_data(\n",
    "    encoder=sparse_autoencoder,\n",
    "    model=model,\n",
    "    tokens=tokens,\n",
    "    cfg=feature_vis_params\n",
    ")\n",
    "\n",
    "feature_data.model = model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for test_idx in feature_data.feature_data_dict:\n",
    "    feature_data.save_feature_centric_vis(\n",
    "        f\"{DASHBOARD_FOLDER}/data_{test_idx:04}.html\",\n",
    "        feature_idx=test_idx,\n",
    "    )\n",
    "    # html_str = feature_data[test_idx].get_all_html()\n",
    "    # with open(f\"data_{test_idx:04}.html\", \"w\") as f:\n",
    "    #     f.write(html_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                # for _, feat_index in enumerate(feature_data.feature_data_dict.keys()):\n",
    "                #     feature = feature_data.feature_data_dict[feat_index]\n",
    "\n",
    "                #     feature_output = {}\n",
    "                #     feature_output[\"featureIndex\"] = feat_index\n",
    "\n",
    "                #     top10_logits = self.round_list(feature.logits_table_data.top_logits)\n",
    "                #     bottom10_logits = self.round_list(\n",
    "                #         feature.logits_table_data.bottom_logits\n",
    "                #     )\n",
    "\n",
    "                #     # TODO: don't precompute/store these. should do it on the frontend\n",
    "                #     max_value = max(\n",
    "                #         np.absolute(bottom10_logits).max(),\n",
    "                #         np.absolute(top10_logits).max(),\n",
    "                #     )\n",
    "                #     neg_bg_values = self.round_list(\n",
    "                #         np.absolute(bottom10_logits) / max_value\n",
    "                #     )\n",
    "                #     pos_bg_values = self.round_list(\n",
    "                #         np.absolute(top10_logits) / max_value\n",
    "                #     )\n",
    "                #     feature_output[\"neg_bg_values\"] = neg_bg_values\n",
    "                #     feature_output[\"pos_bg_values\"] = pos_bg_values\n",
    "\n",
    "                #     if feature.feature_tables_data:\n",
    "                #         feature_output[\"neuron_alignment_indices\"] = (\n",
    "                #             feature.feature_tables_data.neuron_alignment_indices\n",
    "                #         )\n",
    "                #         feature_output[\"neuron_alignment_values\"] = self.round_list(\n",
    "                #             feature.feature_tables_data.neuron_alignment_values\n",
    "                #         )\n",
    "                #         feature_output[\"neuron_alignment_l1\"] = self.round_list(\n",
    "                #             feature.feature_tables_data.neuron_alignment_l1\n",
    "                #         )\n",
    "                #         feature_output[\"correlated_neurons_indices\"] = (\n",
    "                #             feature.feature_tables_data.correlated_neurons_indices\n",
    "                #         )\n",
    "                #         # TODO: this value doesn't exist in the new output type, commenting out for now\n",
    "                #         # there is a cossim value though - is that what's needed?\n",
    "                #         # feature_output[\"correlated_neurons_l1\"] = self.round_list(\n",
    "                #         #     feature.feature_tables_data.correlated_neurons_l1\n",
    "                #         # )\n",
    "                #         feature_output[\"correlated_neurons_pearson\"] = self.round_list(\n",
    "                #             feature.feature_tables_data.correlated_neurons_pearson\n",
    "                #         )\n",
    "                #         # feature_output[\"correlated_features_indices\"] = (\n",
    "                #         #     feature.feature_tables_data.correlated_features_indices\n",
    "                #         # )\n",
    "                #         # feature_output[\"correlated_features_l1\"] = self.round_list(\n",
    "                #         #     feature.feature_tables_data.correlated_features_l1\n",
    "                #         # )\n",
    "                #         # feature_output[\"correlated_features_pearson\"] = self.round_list(\n",
    "                #         #     feature.feature_tables_data.correlated_features_pearson\n",
    "                #         # )\n",
    "\n",
    "                #     feature_output[\"neg_str\"] = self.to_str_tokens_safe(\n",
    "                #         vocab_dict, feature.logits_table_data.bottom_token_ids\n",
    "                #     )\n",
    "                #     feature_output[\"neg_values\"] = bottom10_logits\n",
    "                #     feature_output[\"pos_str\"] = self.to_str_tokens_safe(\n",
    "                #         vocab_dict, feature.logits_table_data.top_token_ids\n",
    "                #     )\n",
    "                #     feature_output[\"pos_values\"] = top10_logits\n",
    "\n",
    "                #     # TODO: don't know what this should be in the new version\n",
    "                #     # feature_output[\"frac_nonzero\"] = (\n",
    "                #     #     feature.middle_plots_data.frac_nonzero\n",
    "                #     # )\n",
    "\n",
    "                #     freq_hist_data = feature.acts_histogram_data\n",
    "                #     freq_bar_values = self.round_list(freq_hist_data.bar_values)\n",
    "                #     feature_output[\"freq_hist_data_bar_values\"] = freq_bar_values\n",
    "                #     feature_output[\"freq_hist_data_tick_vals\"] = self.round_list(\n",
    "                #         freq_hist_data.tick_vals\n",
    "                #     )\n",
    "\n",
    "                #     # TODO: don't precompute/store these. should do it on the frontend\n",
    "                #     freq_bar_values_clipped = [\n",
    "                #         (0.4 * max(freq_bar_values) + 0.6 * v) / max(freq_bar_values)\n",
    "                #         for v in freq_bar_values\n",
    "                #     ]\n",
    "                #     freq_bar_colors = [\n",
    "                #         colors.rgb2hex(BG_COLOR_MAP(v)) for v in freq_bar_values_clipped\n",
    "                #     ]\n",
    "                #     feature_output[\"freq_hist_data_bar_heights\"] = self.round_list(\n",
    "                #         freq_hist_data.bar_heights\n",
    "                #     )\n",
    "                #     feature_output[\"freq_bar_colors\"] = freq_bar_colors\n",
    "\n",
    "                #     logits_hist_data = feature.logits_histogram_data\n",
    "                #     feature_output[\"logits_hist_data_bar_heights\"] = self.round_list(\n",
    "                #         logits_hist_data.bar_heights\n",
    "                #     )\n",
    "                #     feature_output[\"logits_hist_data_bar_values\"] = self.round_list(\n",
    "                #         logits_hist_data.bar_values\n",
    "                #     )\n",
    "                #     feature_output[\"logits_hist_data_tick_vals\"] = self.round_list(\n",
    "                #         logits_hist_data.tick_vals\n",
    "                #     )\n",
    "\n",
    "                #     # TODO: check this\n",
    "                #     feature_output[\"num_tokens_for_dashboard\"] = (\n",
    "                #         self.n_prompts_to_select\n",
    "                #     )\n",
    "\n",
    "                #     activations = []\n",
    "                #     sdbs = feature.sequence_data\n",
    "                #     for sgd in sdbs.seq_group_data:\n",
    "                #         for sd in sgd.seq_data:\n",
    "                #             if (\n",
    "                #                 sd.top_token_ids is not None\n",
    "                #                 and sd.bottom_token_ids is not None\n",
    "                #                 and sd.top_logits is not None\n",
    "                #                 and sd.bottom_logits is not None\n",
    "                #             ):\n",
    "                #                 activation = {}\n",
    "                #                 strs = []\n",
    "                #                 posContribs = []\n",
    "                #                 negContribs = []\n",
    "                #                 for i in range(len(sd.token_ids)):\n",
    "                #                     strs.append(\n",
    "                #                         self.to_str_tokens_safe(\n",
    "                #                             vocab_dict, sd.token_ids[i]\n",
    "                #                         )\n",
    "                #                     )\n",
    "                #                     posContrib = {}\n",
    "                #                     posTokens = [\n",
    "                #                         self.to_str_tokens_safe(vocab_dict, j)\n",
    "                #                         for j in sd.top_token_ids[i]\n",
    "                #                     ]\n",
    "                #                     if len(posTokens) > 0:\n",
    "                #                         posContrib[\"t\"] = posTokens\n",
    "                #                         posContrib[\"v\"] = self.round_list(\n",
    "                #                             sd.top_logits[i]\n",
    "                #                         )\n",
    "                #                     posContribs.append(posContrib)\n",
    "                #                     negContrib = {}\n",
    "                #                     negTokens = [\n",
    "                #                         self.to_str_tokens_safe(vocab_dict, j)  # type: ignore\n",
    "                #                         for j in sd.bottom_token_ids[i]\n",
    "                #                     ]\n",
    "                #                     if len(negTokens) > 0:\n",
    "                #                         negContrib[\"t\"] = negTokens\n",
    "                #                         negContrib[\"v\"] = self.round_list(\n",
    "                #                             sd.bottom_logits[i]\n",
    "                #                         )\n",
    "                #                     negContribs.append(negContrib)\n",
    "\n",
    "                #                 activation[\"logitContributions\"] = json.dumps(\n",
    "                #                     {\"pos\": posContribs, \"neg\": negContribs}\n",
    "                #                 )\n",
    "                #                 activation[\"tokens\"] = strs\n",
    "                #                 activation[\"values\"] = self.round_list(sd.feat_acts)\n",
    "                #                 activation[\"maxValue\"] = max(activation[\"values\"])\n",
    "                #                 activation[\"lossValues\"] = self.round_list(\n",
    "                #                     sd.loss_contribution\n",
    "                #                 )\n",
    "\n",
    "                #                 activations.append(activation)\n",
    "                #     feature_output[\"activations\"] = activations\n",
    "\n",
    "                #     features_outputs.append(feature_output)\n",
    "\n",
    "                # json_object = json.dumps(features_outputs, cls=NpEncoder)\n",
    "\n",
    "                # with open(\n",
    "                #     f\"{self.neuronpedia_folder}/batch-{feature_batch_count}.json\", \"w\"\n",
    "                # ) as f:\n",
    "                #     f.write(json_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mechinterp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
